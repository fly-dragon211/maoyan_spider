{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](https://docs.scrapy.org/en/latest/_images/scrapy_architecture_02.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "使用Scrapy抓取网站需要四个步骤：\n",
    "\n",
    "- 创建Scrapy项目\n",
    "- 定义Item容器\n",
    "- 编写爬虫\n",
    "- 存储内容\n",
    "\n",
    "\n",
    "## 1.创建项目\n",
    "~~~cmd\n",
    "C:\\Users\\fly_dragon\\Desktop>scrapy startproject Dmoz\n",
    "New Scrapy project 'Dmoz', using template directory 'c:\\d_disk\\anaconda3\\lib\\site-packages\\scrapy\\templates\\project', created in:\n",
    "    C:\\Users\\fly_dragon\\Desktop\\Dmoz\n",
    "\n",
    "You can start your first spider with:\n",
    "    cd Dmoz\n",
    "    scrapy genspider example example.com\n",
    "\n",
    "C:\\Users\\fly_dragon\\Desktop>\n",
    "~~~\n",
    "\n",
    "该命令将会创建包含下列内容的scrapyspider目录:\n",
    "\n",
    "```yaml\n",
    "scrapyspider/\n",
    "    scrapy.cfg\n",
    "    scrapyspider/\n",
    "        __init__.py\n",
    "        items.py\n",
    "        pipelines.py\n",
    "        settings.py\n",
    "        spiders/\n",
    "            __init__.py\n",
    "            ...\n",
    "```\n",
    "这些文件分别是\n",
    "\n",
    " - scrapy.cfg: 项目的配置文件。\n",
    "\n",
    "- scrapyspider/: 该项目的python模块。之后您将在此加入代码。\n",
    "- scrapyspider/items.py: 项目中的item文件。\n",
    "- scrapyspider/pipelines.py: 项目中的pipelines文件。\n",
    "- scrapyspider/settings.py: 项目的设置文件。\n",
    "- scrapyspider/spiders/: 放置spider代码的目录。\n",
    "\n",
    "## 2.Item容器\n",
    "Item是保存爬取到的数据的容器，其使用方法和py字典类似，并且提供了额外保护机制来避免拼写错误导致的未定义字段错误。\n",
    "\n",
    "## 3.编写爬虫\n",
    "编写爬虫类Spider，Spider是用户编写用于从网上爬取数据的类。\n",
    "其包含了一个用于可以下载的初始URL，然后是如何跟进网页中的链接以及如何分析页面的内容，还有提取生成Item的方法。\n",
    "\n",
    "~~~py\n",
    "# dmoz_spider.py\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class DmozSpider(scrapy.Spider):\n",
    "    name = \"dmoz1\"   # 不能重复，是爬虫的名字\n",
    "    allowed_domains = ['dmoz-odp.org']\n",
    "    start_urls = [\n",
    "        'https://dmoz-odp.org/Games/Puzzles/',\n",
    "        'https://dmoz-odp.org/Games/Puzzles/Brain_Teasers/'\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        fileName = response.url.split(\"/\")[-2]\n",
    "        # 保存html文件到目录下\n",
    "        with open(fileName, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在cmd下首先进入爬虫项目文件夹，然后使用命令：\n",
    "\n",
    "scrapy crawl dmoz，这里dmoz是爬虫类名(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
